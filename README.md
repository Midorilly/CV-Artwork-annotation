# Investigating how automatic artwork annotation benefits from textual information
Automatic image annotation is a task that assigns several tags in a limited vocabulary to describe an image; in this study we investigate how the task of automatic artwork annotation could benefit from additional textual input. To this extent, we adopt a contrastive learning method and train three variants of CLIP based on the type of textual input they are trained on. We also compare these variant to a simple multi-label classifier based on ViT-B/16 which instead does not leverage textual data. Generally, our proposed variants outperform ViT-B/16; however, from the evaluation it emerges that both our variants and the competitor do not achieve promising results when it comes to macros. Motivated to find an explanation, we find that the employed dataset contains many instances of erroneous or partial ground-truths, mostly due to the fact that they have been gathered from WikiArt, a project based on voluntary contributions.